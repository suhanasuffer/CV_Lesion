{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ONWOdOZJaKXSoHsmP0vGQU4JPCOOXb8H",
      "authorship_tag": "ABX9TyPaQAshUkFNa5gXsFnJyri8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhanasuffer/CV_Lesion/blob/main/CV_Proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset and libraries"
      ],
      "metadata": {
        "id": "sqnki9bH7Bsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, zipfile, shutil\n",
        "\n",
        "# STEP 1: Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 2: Set paths\n",
        "ZIP_PATH = \"/content/drive/My Drive/ZippedBackups/Processed_Kvasir_labeled_images.zip\"\n",
        "EXTRACTED_DIR = \"/content/drive/My Drive/ZippedBackups/Kvasir_raw\"\n",
        "MERGED_OUTPUT_DIR = \"/content/drive/My Drive/ZippedBackups/Kvasir_merged\""
      ],
      "metadata": {
        "id": "qnPjvqhEpoT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0ee45e-b1c0-43d6-dc9e-c72228302891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# ==== CONFIG ====\n",
        "source_root = \"/content/Drive/My Drive/ZippedBackups/Processes_Kvasir_labeled_images\"     # Root folder with class subfolders\n",
        "target_folder = \"/contetn/Drive/My Drive/ZippedBackups/all_frames\"         # Flat output folder\n",
        "os.makedirs(target_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "R3hNsDuhd-yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "B3-HEGBkeZGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "METADATA_PATH = '/content/drive/MyDrive/CV_Backup/metadata.csv'\n",
        "IMAGE_BASE_PATH = '/content/drive/My Drive/ZippedBackups/Kvasir_merged' # Base path where images are stored\n",
        "FEATURE_EXTRACTOR_PATH = '/content/drive/MyDrive/CV_Backup/feature_extractor_densenet121.pth'\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hdqEmcEJetCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Metadata ---\n",
        "import pandas as pd\n",
        "try:\n",
        "    df_meta = pd.read_csv(METADATA_PATH, delimiter=';')\n",
        "    print(f\"Successfully loaded metadata from {METADATA_PATH}\")\n",
        "    print(\"Metadata columns:\", df_meta.columns.tolist())\n",
        "    print(\"First 5 rows:\\n\", df_meta.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Metadata file not found at {METADATA_PATH}\")\n",
        "    # Exit or raise error if metadata is essential\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading metadata: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXOsAd26extX",
        "outputId": "7e206bb7-d237-4717-abf7-1f5852af2913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded metadata from /content/drive/MyDrive/CV_Backup/metadata.csv\n",
            "Metadata columns: ['filename', 'video_id', 'frame_number', 'finding_category', 'finding_class', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
            "First 5 rows:\n",
            "                      filename          video_id  frame_number  \\\n",
            "0  0728084c8da942d9_22803.jpg  0728084c8da942d9         22803   \n",
            "1  0728084c8da942d9_22804.jpg  0728084c8da942d9         22804   \n",
            "2  0728084c8da942d9_22805.jpg  0728084c8da942d9         22805   \n",
            "3  0728084c8da942d9_22806.jpg  0728084c8da942d9         22806   \n",
            "4  0728084c8da942d9_22807.jpg  0728084c8da942d9         22807   \n",
            "\n",
            "  finding_category        finding_class  x1  y1  x2  y2  x3  y3  x4  y4  \n",
            "0          Luminal  Normal clean mucosa NaN NaN NaN NaN NaN NaN NaN NaN  \n",
            "1          Luminal  Normal clean mucosa NaN NaN NaN NaN NaN NaN NaN NaN  \n",
            "2          Luminal  Normal clean mucosa NaN NaN NaN NaN NaN NaN NaN NaN  \n",
            "3          Luminal  Normal clean mucosa NaN NaN NaN NaN NaN NaN NaN NaN  \n",
            "4          Luminal  Normal clean mucosa NaN NaN NaN NaN NaN NaN NaN NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_meta = df_meta.drop(columns=['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4'])\n",
        "df_meta.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "F789B1fFfBI5",
        "outputId": "a615f23b-905c-4b7f-a924-dfffcb3cd97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     filename          video_id  frame_number  \\\n",
              "0  0728084c8da942d9_22803.jpg  0728084c8da942d9         22803   \n",
              "1  0728084c8da942d9_22804.jpg  0728084c8da942d9         22804   \n",
              "2  0728084c8da942d9_22805.jpg  0728084c8da942d9         22805   \n",
              "3  0728084c8da942d9_22806.jpg  0728084c8da942d9         22806   \n",
              "4  0728084c8da942d9_22807.jpg  0728084c8da942d9         22807   \n",
              "\n",
              "  finding_category        finding_class  \n",
              "0          Luminal  Normal clean mucosa  \n",
              "1          Luminal  Normal clean mucosa  \n",
              "2          Luminal  Normal clean mucosa  \n",
              "3          Luminal  Normal clean mucosa  \n",
              "4          Luminal  Normal clean mucosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-afc42f9e-c4e2-4eaa-9941-46ac39fbec8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>video_id</th>\n",
              "      <th>frame_number</th>\n",
              "      <th>finding_category</th>\n",
              "      <th>finding_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0728084c8da942d9_22803.jpg</td>\n",
              "      <td>0728084c8da942d9</td>\n",
              "      <td>22803</td>\n",
              "      <td>Luminal</td>\n",
              "      <td>Normal clean mucosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0728084c8da942d9_22804.jpg</td>\n",
              "      <td>0728084c8da942d9</td>\n",
              "      <td>22804</td>\n",
              "      <td>Luminal</td>\n",
              "      <td>Normal clean mucosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0728084c8da942d9_22805.jpg</td>\n",
              "      <td>0728084c8da942d9</td>\n",
              "      <td>22805</td>\n",
              "      <td>Luminal</td>\n",
              "      <td>Normal clean mucosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0728084c8da942d9_22806.jpg</td>\n",
              "      <td>0728084c8da942d9</td>\n",
              "      <td>22806</td>\n",
              "      <td>Luminal</td>\n",
              "      <td>Normal clean mucosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0728084c8da942d9_22807.jpg</td>\n",
              "      <td>0728084c8da942d9</td>\n",
              "      <td>22807</td>\n",
              "      <td>Luminal</td>\n",
              "      <td>Normal clean mucosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afc42f9e-c4e2-4eaa-9941-46ac39fbec8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-afc42f9e-c4e2-4eaa-9941-46ac39fbec8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-afc42f9e-c4e2-4eaa-9941-46ac39fbec8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e0f9360b-61ce-43e8-b9ea-4d4cbe172261\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0f9360b-61ce-43e8-b9ea-4d4cbe172261')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e0f9360b-61ce-43e8-b9ea-4d4cbe172261 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_meta",
              "summary": "{\n  \"name\": \"df_meta\",\n  \"rows\": 47248,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47229,\n        \"samples\": [\n          \"7a47e8eacea04e64_51625.jpg\",\n          \"3ada4222967f421d_5606.jpg\",\n          \"d626f4f4a5ac4785_51438.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 43,\n        \"samples\": [\n          \"d7a271f233ba4a40\",\n          \"bc84479c66fe4da6\",\n          \"df6b47bafe5143f5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frame_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16685,\n        \"min\": 27,\n        \"max\": 89099,\n        \"num_unique_values\": 27687,\n        \"samples\": [\n          3826,\n          10095,\n          25849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"finding_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Anatomy\",\n          \"Luminal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"finding_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Blood - hematin\",\n          \"Ulcer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Image Transformations ---\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "b0ECFtT5fPdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# --- Inspect Labels Before Mapping ---\n",
        "print(\"\\nInspecting unique labels found in metadata:\")\n",
        "label_column_name = 'finding_class'\n",
        "\n",
        "if label_column_name not in df_meta.columns:\n",
        "    print(f\"Error: Label column '{label_column_name}' not found in metadata!\")\n",
        "    print(f\"Available columns: {df_meta.columns.tolist()}\")\n",
        "    exit()\n",
        "\n",
        "unique_labels = df_meta[label_column_name].unique()\n",
        "print(f\"Unique values in '{label_column_name}': {np.sort(unique_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_0J94GfaX1",
        "outputId": "e64d92ee-da60-420d-ad62-443eac96c0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inspecting unique labels found in metadata:\n",
            "Unique values in 'finding_class': ['Ampulla of Vater' 'Angiectasia' 'Blood - fresh' 'Blood - hematin'\n",
            " 'Erosion' 'Erythema' 'Foreign Body' 'Ileocecal valve' 'Lymphangiectasia'\n",
            " 'Normal clean mucosa' 'Polyp' 'Pylorus' 'Reduced Mucosal View' 'Ulcer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the Label Mapping ---\n",
        "\n",
        "LABEL_MAP = {\n",
        "    'Normal clean mucosa': 0,\n",
        "    'Ampulla of Vater': 1,\n",
        "    'Angiectasia': 2,\n",
        "    'Blood - fresh': 3,\n",
        "    'Blood - hematin': 3,\n",
        "    'Erosion': 4,\n",
        "    'Erythema': 5,\n",
        "    'Foreign Body':6,\n",
        "    'Ileocecal valve':7,\n",
        "    'Lymphangiectasia': 8,\n",
        "    'Polyp':9,\n",
        "    'Pylorus':10,\n",
        "    'Reduced Mucosal View': 11,\n",
        "    'Ulcer':12\n",
        "\n",
        "}\n",
        "NUM_CLASSES = len(LABEL_MAP)\n",
        "print(f\"\\nDefined LABEL_MAP: {LABEL_MAP}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTOcqEGpfnvX",
        "outputId": "323299d1-0f15-47bb-e253-a37f780f1df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defined LABEL_MAP: {'Normal clean mucosa': 0, 'Ampulla of Vater': 1, 'Angiectasia': 2, 'Blood - fresh': 3, 'Blood - hematin': 3, 'Erosion': 4, 'Erythema': 5, 'Foreign Body': 6, 'Ileocecal valve': 7, 'Lymphangiectasia': 8, 'Polyp': 9, 'Pylorus': 10, 'Reduced Mucosal View': 11, 'Ulcer': 12}\n",
            "Number of classes: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Apply Label Mapping ---\n",
        "df_meta['label_idx'] = df_meta[label_column_name].map(LABEL_MAP)\n",
        "\n",
        "# --- Check for Unmapped Labels ---\n",
        "unmapped_count = df_meta['label_idx'].isnull().sum()\n",
        "if unmapped_count > 0:\n",
        "    print(f\"\\nWarning: {unmapped_count} rows have labels not found in LABEL_MAP.\")\n",
        "    unmapped_values = df_meta[df_meta['label_idx'].isnull()][label_column_name].unique()\n",
        "    print(f\"Labels without mapping: {unmapped_values}\")\n",
        "    print(\"Please update LABEL_MAP or handle these rows.\")\n",
        "else:\n",
        "    print(\"\\nAll labels successfully mapped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPPEHg4lfzq9",
        "outputId": "005eb90d-f002-4118-93d0-c4bedccbcba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All labels successfully mapped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label_idx to integer type\n",
        "df_meta['label_idx'] = df_meta['label_idx'].astype(int)\n",
        "\n",
        "# --- Ensure correct data types for sorting ---\n",
        "df_meta['frame_number'] = df_meta['frame_number'].astype(int)\n",
        "\n",
        "# --- Sort by video and frame order ---\n",
        "df_meta = df_meta.sort_values(by=['video_id', 'frame_number']).reset_index(drop=True)\n",
        "print(\"\\nMetadata sorted by video_id and frame_number.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzdZvPLf8Ut",
        "outputId": "b17e2d7a-59cf-4f16-a50a-8c7394fd1874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata sorted by video_id and frame_number.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---Custom PyTorch Dataset ---\n",
        "class KvasirFrameDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_base_path, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.image_base_path = image_base_path\n",
        "        # --- Pre-check base path existence ---\n",
        "        if not os.path.isdir(self.image_base_path):\n",
        "             print(f\"ERROR: IMAGE_BASE_PATH '{self.image_base_path}' does not exist or is not a directory.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        img_filename = row['filename']\n",
        "        img_path = os.path.join(self.image_base_path, img_filename)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found at {img_path}. Returning black image.\")\n",
        "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black')\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Error loading {img_path}: {e}. Returning black image.\")\n",
        "             image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black')\n",
        "\n",
        "        label = row['label_idx']\n",
        "        video_id = row['video_id']\n",
        "        frame_number = row['frame_number']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, video_id, frame_number"
      ],
      "metadata": {
        "id": "obV-Nm8jgDyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the dataset\n",
        "full_dataset = KvasirFrameDataset(df_meta, IMAGE_BASE_PATH, transform=preprocess)\n",
        "# Create a DataLoader (shuffle=False crucial for sequential processing)\n",
        "frame_dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"\\nDataset and DataLoader created. Number of frames: {len(full_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXN4Xhtxg4SS",
        "outputId": "2fbbf0a9-edf6-4531-ac70-d5409729fd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset and DataLoader created. Number of frames: 47248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "2O4hFzHT7Nwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def load_feature_extractor(model_path, device):\n",
        "    model = models.densenet121(weights=None)\n",
        "\n",
        "    num_ftrs = model.classifier.in_features #no. of features\n",
        "\n",
        "    model.classifier = nn.Identity() # Output features directly, no classification layer\n",
        "\n",
        "    try:\n",
        "         model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "         print(\"Successfully loaded model state_dict.\")\n",
        "    except RuntimeError:\n",
        "         print(\"Loading full state_dict failed. Trying to load with strict=False.\")\n",
        "         try:\n",
        "            state_dict = torch.load(model_path, map_location=device)\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Loaded state_dict with strict=False. Check for missing/unexpected keys.\")\n",
        "         except Exception as e:\n",
        "            print(f\"Error loading state_dict even with strict=False: {e}\")\n",
        "            print(\"Could not load feature extractor weights. Please check the .pth file and model architecture.\")\n",
        "            return None\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Feature extractor loaded on {device}. Output feature dimension: {num_ftrs}\")\n",
        "    return model, num_ftrs"
      ],
      "metadata": {
        "id": "Az9Odj-YhB2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor, feature_dim = load_feature_extractor(FEATURE_EXTRACTOR_PATH, DEVICE)\n",
        "\n",
        "if feature_extractor is None:\n",
        "     raise ValueError(\"Failed to load feature extractor model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TmzjXHliWma",
        "outputId": "b4a05cd4-86d8-46f5-ed99-ebd8efb8ae26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model state_dict.\n",
            "Feature extractor loaded on cpu. Output feature dimension: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "video_features = defaultdict(lambda: {'features': [], 'labels': [], 'frame_ids': []})\n",
        "\n",
        "print(\"Starting feature extraction...\")\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels, video_ids, frame_ids) in enumerate(frame_dataloader):\n",
        "        images = images.to(DEVICE)\n",
        "        features = feature_extractor(images)\n",
        "\n",
        "        # Move features to CPU and store them per video\n",
        "        features_cpu = features.cpu().numpy()\n",
        "        labels_cpu = labels.numpy()\n",
        "        video_ids_cpu = video_ids.numpy() if not isinstance(video_ids[0], str) else video_ids\n",
        "        frame_ids_cpu = frame_ids.numpy()\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            v_id = video_ids_cpu[i]\n",
        "            f_id = frame_ids_cpu[i]\n",
        "            lbl = labels_cpu[i]\n",
        "            feat = features_cpu[i]\n",
        "\n",
        "            video_features[v_id]['features'].append(feat)\n",
        "            video_features[v_id]['labels'].append(lbl)\n",
        "            video_features[v_id]['frame_ids'].append(f_id)\n",
        "\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "             print(f\"Processed batch {batch_idx + 1}/{len(frame_dataloader)}\")\n",
        "\n",
        "print(\"Feature extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjUhP_0jiacd",
        "outputId": "dcaf311c-b986-4f51-894f-fe8f211f284f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature extraction...\n",
            "Warning: Error loading /content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_4382.jpg: [Errno 5] Input/output error: '/content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_4382.jpg'. Returning black image.\n",
            "Warning: Error loading /content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_887.jpg: [Errno 5] Input/output error: '/content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_887.jpg'. Returning black image.Warning: Error loading /content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_11227.jpg: [Errno 5] Input/output error: '/content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_11227.jpg'. Returning black image.\n",
            "Warning: Error loading /content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_5628.jpg: [Errno 5] Input/output error: '/content/drive/My Drive/ZippedBackups/Kvasir_merged/04a78ef00c5245e0_5628.jpg'. Returning black image.\n",
            "\n",
            "Processed batch 50/1477\n",
            "Processed batch 100/1477\n",
            "Processed batch 150/1477\n",
            "Processed batch 200/1477\n",
            "Processed batch 250/1477\n",
            "Processed batch 300/1477\n",
            "Processed batch 350/1477\n",
            "Processed batch 400/1477\n",
            "Processed batch 450/1477\n",
            "Processed batch 500/1477\n",
            "Processed batch 550/1477\n",
            "Processed batch 600/1477\n",
            "Processed batch 650/1477\n",
            "Processed batch 700/1477\n",
            "Processed batch 750/1477\n",
            "Processed batch 800/1477\n",
            "Processed batch 850/1477\n",
            "Processed batch 900/1477\n",
            "Processed batch 950/1477\n",
            "Processed batch 1000/1477\n",
            "Processed batch 1050/1477\n",
            "Processed batch 1100/1477\n",
            "Processed batch 1150/1477\n",
            "Processed batch 1200/1477\n",
            "Processed batch 1250/1477\n",
            "Processed batch 1300/1477\n",
            "Processed batch 1350/1477\n",
            "Processed batch 1400/1477\n",
            "Processed batch 1450/1477\n",
            "Feature extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists of features/labels/frame_ids to numpy arrays for easier handling\n",
        "for v_id in video_features:\n",
        "    video_features[v_id]['features'] = np.array(video_features[v_id]['features'])\n",
        "    video_features[v_id]['labels'] = np.array(video_features[v_id]['labels'])\n",
        "    video_features[v_id]['frame_ids'] = np.array(video_features[v_id]['frame_ids'])\n",
        "    # Ensure sorting just in case dataloader ordering wasn't perfect (though shuffle=False should guarantee it)\n",
        "    sort_indices = np.argsort(video_features[v_id]['frame_ids'])\n",
        "    video_features[v_id]['features'] = video_features[v_id]['features'][sort_indices]\n",
        "    video_features[v_id]['labels'] = video_features[v_id]['labels'][sort_indices]\n",
        "    video_features[v_id]['frame_ids'] = video_features[v_id]['frame_ids'][sort_indices]"
      ],
      "metadata": {
        "id": "GBrLHk6di1_D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selective Frame Sampling"
      ],
      "metadata": {
        "id": "d__9RfUIVY2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "import numpy as np\n",
        "\n",
        "def selective_frame_sampling(features, frame_ids, method='cosine', threshold=0.95):\n",
        "\n",
        "    if len(features) <= 1:\n",
        "        return np.arange(len(features)), features, frame_ids\n",
        "\n",
        "    selected_indices = [0]\n",
        "    selected_features_list = [features[0]]\n",
        "    selected_frame_ids_list = [frame_ids[0]]\n",
        "    last_selected_feature = features[0]\n",
        "\n",
        "    for i in range(1, len(features)):\n",
        "        current_feature = features[i]\n",
        "        keep_frame = False\n",
        "\n",
        "        if method == 'cosine':\n",
        "            sim = cosine_similarity(last_selected_feature.reshape(1, -1), current_feature.reshape(1, -1))[0, 0]\n",
        "            if sim < threshold:\n",
        "                keep_frame = True\n",
        "        elif method == 'euclidean':\n",
        "            dist = euclidean_distances(last_selected_feature.reshape(1, -1), current_feature.reshape(1, -1))[0, 0]\n",
        "            if dist > threshold:\n",
        "                keep_frame = True\n",
        "        else:\n",
        "            raise ValueError(\"Method must be 'cosine' or 'euclidean'\")\n",
        "\n",
        "        if keep_frame:\n",
        "            selected_indices.append(i)\n",
        "            selected_features_list.append(current_feature)\n",
        "            selected_frame_ids_list.append(frame_ids[i])\n",
        "            last_selected_feature = current_feature\n",
        "\n",
        "    selected_features = np.array(selected_features_list)\n",
        "    selected_frame_ids = np.array(selected_frame_ids_list)\n",
        "\n",
        "    return np.array(selected_indices), selected_features, selected_frame_ids"
      ],
      "metadata": {
        "id": "g9fnCLOJVWDp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "video_id_to_process = list(video_features.keys())[0] # Get the first video ID\n",
        "original_feats = video_features[video_id_to_process]['features']\n",
        "original_fids = video_features[video_id_to_process]['frame_ids']\n",
        "\n",
        "print(f\"Video {video_id_to_process}: Original frames = {len(original_feats)}\")\n",
        "\n",
        "# # Sample using Cosine Similarity (keep if similarity < 0.95)\n",
        "sel_indices_cos, sel_feats_cos, sel_fids_cos = selective_frame_sampling(\n",
        "    original_feats, original_fids, method='cosine', threshold=0.95\n",
        ")\n",
        "print(f\"Cosine Sampling (thresh=0.98): Selected frames = {len(sel_feats_cos)}\")\n",
        "\n",
        "# # Sample using Euclidean Distance (keep if distance > 5.0 - threshold depends heavily on feature scale)\n",
        "sel_indices_euc, sel_feats_euc, sel_fids_euc = selective_frame_sampling(\n",
        "    original_feats, original_fids, method='euclidean', threshold=5.0\n",
        ")\n",
        "print(f\"Euclidean Sampling (thresh=5.0): Selected frames = {len(sel_feats_euc)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC9avfD5Vjcn",
        "outputId": "0cb272fe-d0cc-4593-a87d-2161fd75a15b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 04a78ef00c5245e0: Original frames = 1292\n",
            "Cosine Sampling (thresh=0.98): Selected frames = 611\n",
            "Euclidean Sampling (thresh=5.0): Selected frames = 1150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model definition"
      ],
      "metadata": {
        "id": "oBa6HHjRV1Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LesionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out.reshape(-1, out.shape[2]))\n",
        "        out = out.view(x.size(0), x.size(1), -1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-Xee3-upVpZe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameters for LSTM ---\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 2\n",
        "DROPOUT = 0.5"
      ],
      "metadata": {
        "id": "rz9Tqj3EWL7c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Instantiate the temporal model ---\n",
        "temporal_model = LesionLSTM(feature_dim, HIDDEN_DIM, LSTM_LAYERS, NUM_CLASSES, DROPOUT).to(DEVICE)\n",
        "\n",
        "print(temporal_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFpgmAc0WNc5",
        "outputId": "cf6c2e9c-0e1a-48f6-c691-686d7bb3b34f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LesionLSTM(\n",
            "  (lstm): LSTM(1024, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=14, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "mDPBFartWRua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, video_feature_dict, use_sampling=False, sampling_params=None):\n",
        "        self.video_ids = list(video_feature_dict.keys())\n",
        "        self.video_data = video_feature_dict\n",
        "        self.use_sampling = use_sampling\n",
        "        self.sampling_params = sampling_params if sampling_params else {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        data = self.video_data[video_id]\n",
        "        features = data['features']\n",
        "        labels = data['labels']\n",
        "        frame_ids = data['frame_ids']\n",
        "\n",
        "        if self.use_sampling:\n",
        "            _, sampled_features, sampled_frame_ids = selective_frame_sampling(\n",
        "                features, frame_ids, **self.sampling_params\n",
        "            )\n",
        "            frame_id_to_label = {fid: lbl for fid, lbl in zip(frame_ids, labels)}\n",
        "            sampled_labels = np.array([frame_id_to_label[fid] for fid in sampled_frame_ids])\n",
        "\n",
        "            features = sampled_features\n",
        "            labels = sampled_labels\n",
        "            frame_ids = sampled_frame_ids\n",
        "\n",
        "        # Convert numpy arrays to tensors\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "        frame_ids_tensor = torch.tensor(frame_ids, dtype=torch.long)\n",
        "\n",
        "        return features_tensor, labels_tensor, frame_ids_tensor, video_id"
      ],
      "metadata": {
        "id": "MI6RLtGKWPDR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... video_features now holds numpy arrays ...\n",
        "\n",
        "print(\"\\nChecking for empty video sequences...\")\n",
        "feature_dim = 1024\n",
        "non_empty_video_features = {}\n",
        "empty_video_ids = []\n",
        "total_videos = len(video_features)\n",
        "\n",
        "for v_id, data in video_features.items():\n",
        "    features_array = data['features']\n",
        "    # Check if the array is actually empty or has zero frames\n",
        "    if features_array.shape[0] > 0:\n",
        "         # Optional but good: Check if feature dimension is correct\n",
        "         if len(features_array.shape) == 2 and features_array.shape[1] == feature_dim:\n",
        "              non_empty_video_features[v_id] = data\n",
        "         else:\n",
        "              print(f\"WARNING: Video {v_id} has unexpected feature shape {features_array.shape}. Skipping.\")\n",
        "              empty_video_ids.append(v_id)\n",
        "    else:\n",
        "        print(f\"WARNING: Video {v_id} has 0 frames/features after extraction. Skipping.\")\n",
        "        empty_video_ids.append(v_id)\n",
        "\n",
        "print(f\"Removed {len(empty_video_ids)} videos with empty or malformed features out of {total_videos}.\")\n",
        "print(f\"Proceeding with {len(non_empty_video_features)} videos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhqiRXNnWlKg",
        "outputId": "7429e718-a172-48bb-bd96-b6556e890077"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking for empty video sequences...\n",
            "WARNING: Video 04a78ef00c5245e0_11227.jpg has 0 frames/features after extraction. Skipping.\n",
            "Removed 1 videos with empty or malformed features out of 44.\n",
            "Proceeding with 43 videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collate function\n"
      ],
      "metadata": {
        "id": "vJsoTXhaXWXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Collate function for padding sequences ---\n",
        "def collate_fn(batch):\n",
        "    # batch is a list of tuples: [(feats1, labels1, fids1, vid1), (feats2, labels2, fids2, vid2), ...]\n",
        "    features_list, labels_list, frame_ids_list, video_ids = zip(*batch)\n",
        "\n",
        "    # Pad sequences: batch_first=True means output shape (batch_size, max_seq_len, feature_dim)\n",
        "    features_padded = pad_sequence(features_list, batch_first=True, padding_value=0.0)\n",
        "    # Pad labels: Use a value like -100 for padding for CrossEntropyLoss ignore_index\n",
        "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=-100)\n",
        "    # Pad frame_ids (optional, but useful for tracking)\n",
        "    frame_ids_padded = pad_sequence(frame_ids_list, batch_first=True, padding_value=-1) # Use -1 for padding ID\n",
        "\n",
        "    # Also return sequence lengths for potential use with pack_padded_sequence (optimizes RNN computation)\n",
        "    lengths = torch.tensor([len(f) for f in features_list])\n",
        "\n",
        "    return features_padded, labels_padded, frame_ids_padded, lengths, video_ids"
      ],
      "metadata": {
        "id": "BxwsIHNOW4nq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train epoch definition"
      ],
      "metadata": {
        "id": "ItUREdwmYJfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = optim.Adam(temporal_model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "N6H7LxcwYMgq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for features, labels, _, lengths, _ in dataloader: # Ignore frame_ids, video_ids for loss calc\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features) # Output: (batch, seq_len, num_classes)\n",
        "\n",
        "        # Reshape for CrossEntropyLoss: (batch * seq_len, num_classes)\n",
        "        # Labels: (batch, seq_len) -> (batch * seq_len)\n",
        "        loss = criterion(outputs.view(-1, NUM_CLASSES), labels.view(-1))\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * features.size(0) # Loss per batch * batch size\n",
        "        total_samples += features.size(0) # Count number of videos processed\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "oiTrkeWIYSW9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation definiton"
      ],
      "metadata": {
        "id": "tqHUFmyPo3CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "def evaluate_model(model, dataloader, device, num_classes, frame_rate=30):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_video_ids = []\n",
        "    all_frame_ids = []\n",
        "    all_probs = []\n",
        "\n",
        "    # Store results per video for easier analysis and timestamp generation\n",
        "    results_per_video = defaultdict(lambda: {'preds': [], 'labels': [], 'frame_ids': [], 'probs': []})\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels, frame_ids_padded, lengths, video_ids_batch in dataloader:\n",
        "            features = features.to(device)\n",
        "            outputs = model(features) # (batch, seq_len, num_classes)\n",
        "            probs = torch.softmax(outputs, dim=2) # Get probabilities\n",
        "            preds = torch.argmax(probs, dim=2)    # Get predicted class index\n",
        "\n",
        "            # Move results to CPU for processing\n",
        "            preds_cpu = preds.cpu().numpy()\n",
        "            probs_cpu = probs.cpu().numpy()\n",
        "            labels_cpu = labels.numpy()\n",
        "            frame_ids_cpu = frame_ids_padded.numpy()\n",
        "\n",
        "            # Process results sequence by sequence in the batch\n",
        "            for i in range(features.size(0)): # Iterate through batch items\n",
        "                seq_len = lengths[i].item() # Get original sequence length before padding\n",
        "                video_id = video_ids_batch[i]\n",
        "\n",
        "                # Get valid (non-padded) predictions, labels, frame_ids, probs for this sequence\n",
        "                valid_preds = preds_cpu[i, :seq_len]\n",
        "                valid_labels = labels_cpu[i, :seq_len]\n",
        "                valid_frame_ids = frame_ids_cpu[i, :seq_len]\n",
        "                valid_probs = probs_cpu[i, :seq_len, :]\n",
        "\n",
        "                # Append to overall lists (for flat metrics)\n",
        "                all_preds.extend(valid_preds)\n",
        "                all_labels.extend(valid_labels)\n",
        "                all_frame_ids.extend(valid_frame_ids) # Keep track of frame_id for each prediction\n",
        "                all_video_ids.extend([video_id] * seq_len) # Keep track of video_id\n",
        "                all_probs.extend(valid_probs)\n",
        "\n",
        "                # Store results grouped by video\n",
        "                results_per_video[video_id]['preds'].extend(valid_preds)\n",
        "                results_per_video[video_id]['labels'].extend(valid_labels)\n",
        "                results_per_video[video_id]['frame_ids'].extend(valid_frame_ids)\n",
        "                results_per_video[video_id]['probs'].extend(valid_probs)\n",
        "\n",
        "\n",
        "    # --- Calculate Frame-Level Metrics ---\n",
        "    valid_indices = [i for i, lbl in enumerate(all_labels) if lbl >= 0]\n",
        "    all_preds = np.array(all_preds)[valid_indices]\n",
        "    all_labels = np.array(all_labels)[valid_indices]\n",
        "    all_probs = np.array(all_probs)[valid_indices]\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro', zero_division=0)\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not calculate ROC AUC: {e}. Check if all classes are present in predictions/labels.\")\n",
        "         roc_auc = float('nan')\n",
        "\n",
        "    print(\"\\n--- Frame-Level Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "    print(f\"Micro Precision: {precision_micro:.4f}, Recall: {recall_micro:.4f}, F1-Score: {f1_micro:.4f}\")\n",
        "    print(f\"Weighted ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "    # --- Identify Lesion Timestamps ---\n",
        "    detected_lesions = [] # List of tuples: (video_id, frame_id, timestamp_sec, predicted_class_index)\n",
        "    inv_label_map = {v: k for k, v in LABEL_MAP.items()}\n",
        "\n",
        "    for video_id, data in results_per_video.items():\n",
        "        frame_ids = data['frame_ids']\n",
        "        preds = data['preds']\n",
        "        for i in range(len(preds)):\n",
        "            pred_class_idx = preds[i]\n",
        "            # Check if the prediction is NOT the 'negative' class (assuming index 0)\n",
        "            if pred_class_idx != LABEL_MAP['Normal clean mucosa']:\n",
        "                frame_id = frame_ids[i]\n",
        "                timestamp_sec = frame_id / frame_rate\n",
        "                predicted_class_name = inv_label_map.get(pred_class_idx, 'Unknown')\n",
        "                detected_lesions.append({\n",
        "                    'video_id': video_id,\n",
        "                    'frame_id': frame_id,\n",
        "                    'timestamp_sec': timestamp_sec,\n",
        "                    'predicted_class_idx': pred_class_idx,\n",
        "                    'predicted_class_name': predicted_class_name\n",
        "                })\n",
        "\n",
        "    print(f\"\\n--- Detected Lesions ---\")\n",
        "    for detection in detected_lesions[:15]:\n",
        "         print(f\"  Video: {detection['video_id']}, Frame: {detection['frame_id']}, \"\n",
        "               f\"Time: {detection['timestamp_sec']:.2f}s, Class: {detection['predicted_class_name']}\")\n",
        "    if not detected_lesions:\n",
        "         print(\"  No lesions detected (or 'negative' class ID is incorrect).\")\n",
        "\n",
        "    return detected_lesions, results_per_video"
      ],
      "metadata": {
        "id": "86ELAGIoo5zN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RRR"
      ],
      "metadata": {
        "id": "qnE37x56qTWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rrr(total_original_frames, total_sampled_frames):\n",
        "     if total_original_frames == 0:\n",
        "         return 0.0\n",
        "     return 1.0 - (total_sampled_frames / total_original_frames)"
      ],
      "metadata": {
        "id": "vZK7o97tqGs3"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDR"
      ],
      "metadata": {
        "id": "V1gXW608qUGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "negative_label_index = LABEL_MAP.get('Normal clean mucosa')\n",
        "\n",
        "# Mark a video as True (positive) if at least one of its frames has a lesion label\n",
        "ground_truth_video_labels = defaultdict(bool)\n",
        "for _, row in df_meta.iterrows():\n",
        "    if row['label_idx'] != negative_label_index:\n",
        "        ground_truth_video_labels[row['video_id']] = True\n",
        "\n",
        "ground_truth_video_labels = dict(ground_truth_video_labels)\n"
      ],
      "metadata": {
        "id": "rqjm5BAkqSty"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict # Make sure it's imported\n",
        "\n",
        "def calculate_ldr(results_per_video, ground_truth_video_labels, label_map):\n",
        "    gt_positive_videos = {vid for vid, has_lesion in ground_truth_video_labels.items() if has_lesion}\n",
        "    if not gt_positive_videos:\n",
        "        print(\"Warning: No ground truth positive videos found for LDR calculation.\")\n",
        "        return 0.0\n",
        "\n",
        "    detected_gt_positive_videos = 0\n",
        "    negative_class_index = label_map.get('Normal clean mucosa', label_map.get('Normal clean mucosa', 0))\n",
        "\n",
        "    for video_id in gt_positive_videos:\n",
        "        if video_id in results_per_video:\n",
        "            # Check if any frame in this GT positive video was predicted as a lesion\n",
        "            predicted_classes = results_per_video[video_id]['preds']\n",
        "            if any(pred_class != negative_class_index for pred_class in predicted_classes):\n",
        "                detected_gt_positive_videos += 1\n",
        "\n",
        "    ldr = detected_gt_positive_videos / len(gt_positive_videos)\n",
        "    return ldr"
      ],
      "metadata": {
        "id": "qYsB5CbkqfPE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations definition"
      ],
      "metadata": {
        "id": "qq9sDxrbrxYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Helper function to get original frame ---\n",
        "def get_original_frame(video_id, frame_number, base_path, metadata_df):\n",
        "    row = metadata_df[(metadata_df['video_id'] == video_id) & (metadata_df['frame_number'] == frame_number)]\n",
        "    if row.empty:\n",
        "        print(f\"Warning: Could not find metadata for {video_id}, frame {frame_number}\")\n",
        "        return None\n",
        "\n",
        "    img_filename = row.iloc[0]['filename']\n",
        "    video_id_str = str(video_id)\n",
        "    img_path = os.path.join(base_path, img_filename)\n",
        "\n",
        "    frame = cv2.imread(img_path)\n",
        "    if frame is None:\n",
        "        print(f\"Warning: Failed to read image at {img_path}\")\n",
        "    return frame"
      ],
      "metadata": {
        "id": "HnnchAvGr2rD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to draw predictions on a frame ---\n",
        "def draw_prediction_on_frame(frame, frame_number, timestamp_sec, pred_class_idx, pred_class_name, is_lesion):\n",
        "    if frame is None:\n",
        "        return None\n",
        "\n",
        "    # Text settings\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.6\n",
        "    thickness = 1\n",
        "    text_color = (0, 0, 255) if is_lesion else (0, 255, 0) # Red for lesion, Green for normal\n",
        "    bg_color = (255, 255, 255)\n",
        "\n",
        "    # Timestamp text\n",
        "    ts_text = f\"Frame: {frame_number} | Time: {timestamp_sec:.2f}s\"\n",
        "    (ts_w, ts_h), _ = cv2.getTextSize(ts_text, font, font_scale, thickness)\n",
        "    cv2.rectangle(frame, (5, 5), (5 + ts_w, 5 + ts_h + 5), bg_color, -1)\n",
        "    cv2.putText(frame, ts_text, (5, 5 + ts_h), font, font_scale, (0,0,0), thickness, cv2.LINE_AA)\n",
        "\n",
        "    # Prediction text\n",
        "    pred_text = f\"Pred: {pred_class_name} ({pred_class_idx})\"\n",
        "    (pred_w, pred_h), _ = cv2.getTextSize(pred_text, font, font_scale, thickness)\n",
        "    cv2.rectangle(frame, (5, 15 + ts_h), (5 + pred_w, 15 + ts_h + pred_h + 5), bg_color, -1)\n",
        "    cv2.putText(frame, pred_text, (5, 15 + ts_h + pred_h), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
        "\n",
        "    return frame"
      ],
      "metadata": {
        "id": "TzGvCKWCsGl-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_detections(detected_lesions_list, base_image_path, metadata_df, output_dir=\"visualization\", frame_rate=30):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"\\nGenerating visualizations for {len(detected_lesions_list)} detected lesion frames...\")\n",
        "\n",
        "    DISPLAY_NAME_MAP = {\n",
        "         0: 'Normal clean mucosa',\n",
        "         1: 'Ampulla of Vater',\n",
        "         2: 'Angiectasia',\n",
        "         3: 'Bleeding',\n",
        "         4: 'Bleeding',\n",
        "         5: 'Erosion' ,\n",
        "         6: 'Erythema',\n",
        "         7: 'Foreign Body',\n",
        "         8: 'Ileocecal valve',\n",
        "         9: 'Lymphangiectasia',\n",
        "         10: 'Polyp',\n",
        "         11: 'Pylorus',\n",
        "         12: 'Reduced Mucosal View',\n",
        "         13: 'Ulcer'\n",
        "\n",
        "    }\n",
        "    count = 0\n",
        "    for detection in detected_lesions_list:\n",
        "        video_id = detection['video_id']\n",
        "        frame_number = detection['frame_id']\n",
        "        pred_class_idx = detection['predicted_class_idx']\n",
        "        timestamp_sec = detection['timestamp_sec']\n",
        "\n",
        "        pred_class_name = DISPLAY_NAME_MAP.get(pred_class_idx, f\"Unknown ({pred_class_idx})\")\n",
        "\n",
        "        original_frame = get_original_frame(video_id, frame_number, base_image_path, metadata_df)\n",
        "        if original_frame is not None:\n",
        "            annotated_frame = draw_prediction_on_frame(\n",
        "                original_frame.copy(),\n",
        "                frame_number,\n",
        "                timestamp_sec,\n",
        "                pred_class_idx,\n",
        "                pred_class_name,\n",
        "                is_lesion=True )\n",
        "\n",
        "            # Save the annotated frame\n",
        "            output_filename = f\"detection_{video_id}_frame_{frame_number}_{pred_class_name}.jpg\"\n",
        "            output_path = os.path.join(output_dir, output_filename)\n",
        "            cv2.imwrite(output_path, annotated_frame)\n",
        "            count += 1\n",
        "            if count % 50 == 0: # Print progress\n",
        "                 print(f\"  Saved {count} annotated frames...\")\n",
        "\n",
        "    print(f\"Finished saving {count} annotated lesion frames to '{output_dir}'.\")"
      ],
      "metadata": {
        "id": "XcuRMJZJsOUv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate Summary Report ---\n",
        "def create_summary_report(detected_lesions_list, output_csv=\"detection_summary_cv.csv\"):\n",
        "    if not detected_lesions_list:\n",
        "        print(\"No lesions detected to create a summary report.\")\n",
        "        return\n",
        "\n",
        "    # Use the same display name mapping as in visualization\n",
        "    DISPLAY_NAME_MAP = {\n",
        "         0: 'Normal clean mucosa', 1: 'Ampulla of Vater', 2: 'Angiectasia', 3: 'Bleeding', 4: 'Bleeding', 5: 'Erosion' , 6: 'Erythema',\n",
        "         7: 'Foreign Body', 8: 'Ileocecal valve', 9: 'Lymphangiectasia', 10: 'Polyp', 11: 'Pylorus', 12: 'Reduced Mucosal View', 13: 'Ulcer'\n",
        "    }\n",
        "\n",
        "    report_data = []\n",
        "    for detection in detected_lesions_list:\n",
        "         report_entry = detection.copy() # Start with existing data\n",
        "         pred_idx = report_entry['predicted_class_idx']\n",
        "         report_entry['predicted_class_name'] = DISPLAY_NAME_MAP.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "         report_data.append(report_entry)\n",
        "\n",
        "\n",
        "    df_report = pd.DataFrame(report_data)\n",
        "    df_report = df_report[['video_id', 'frame_id', 'timestamp_sec', 'predicted_class_idx', 'predicted_class_name']]\n",
        "    df_report.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved detection summary report to '{output_csv}'.\")"
      ],
      "metadata": {
        "id": "gM4tEavus9dI"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoader"
      ],
      "metadata": {
        "id": "lToI9tiwXxU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Sampling"
      ],
      "metadata": {
        "id": "PHRiv86iX4sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset without sampling\n",
        "temporal_dataset_no_sampling = VideoFeatureDataset(non_empty_video_features, use_sampling=False)"
      ],
      "metadata": {
        "id": "HMzVvYn6XUR5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders\n",
        "# Adjust batch size for temporal model based on sequence lengths and memory\n",
        "TEMPORAL_BATCH_SIZE = 8\n",
        "dataloader_no_sampling = DataLoader(\n",
        "    temporal_dataset_no_sampling,\n",
        "    batch_size=TEMPORAL_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Temporal Dataloader (No Sampling): {len(dataloader_no_sampling)} batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH4Qy0U2X99X",
        "outputId": "ebbad050-6087-4393-fdcf-d907e0607c4e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporal Dataloader (No Sampling): 6 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Loop ---\n",
        "print(\"Starting Temporal Model Training...\")\n",
        "\n",
        "train_dataloader = dataloader_no_sampling\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = train_epoch(temporal_model, train_dataloader, criterion, optimizer, DEVICE)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Save the trained temporal model\n",
        "torch.save(temporal_model.state_dict(), 'temporal_model_trained_wo_sampling.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLeoenY7YFdh",
        "outputId": "0309f544-acd3-40ab-ca43-05a24f733bca"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Temporal Model Training...\n",
            "Epoch 1/20, Loss: 0.7590\n",
            "Epoch 2/20, Loss: 0.6740\n",
            "Epoch 3/20, Loss: 0.5580\n",
            "Epoch 4/20, Loss: 0.5526\n",
            "Epoch 5/20, Loss: 0.5072\n",
            "Epoch 6/20, Loss: 0.4610\n",
            "Epoch 7/20, Loss: 0.4586\n",
            "Epoch 8/20, Loss: 0.5301\n",
            "Epoch 9/20, Loss: 0.5010\n",
            "Epoch 10/20, Loss: 0.4216\n",
            "Epoch 11/20, Loss: 0.4049\n",
            "Epoch 12/20, Loss: 0.4230\n",
            "Epoch 13/20, Loss: 0.3652\n",
            "Epoch 14/20, Loss: 0.3808\n",
            "Epoch 15/20, Loss: 0.3641\n",
            "Epoch 16/20, Loss: 0.2978\n",
            "Epoch 17/20, Loss: 0.3266\n",
            "Epoch 18/20, Loss: 0.3769\n",
            "Epoch 19/20, Loss: 0.3308\n",
            "Epoch 20/20, Loss: 0.3026\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('temporal_model_trained_wo_sampling.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uDyjvCiaYgQP",
        "outputId": "0503e169-4ed5-427e-de37-0b764cb94300"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16d5b9f4-89cf-49b9-a151-d5b44c5dad1c\", \"temporal_model_trained_wo_sampling.pth\", 16845298)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run Evaluation ---\n",
        "print(\"Evaluating without Selective Sampling\")\n",
        "\n",
        "# Need a dataloader with shuffle=False for consistent evaluation order if needed\n",
        "eval_dataloader_no_sampling = DataLoader(\n",
        "    temporal_dataset_no_sampling, batch_size=TEMPORAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "detections_no_sampling, results_per_video_no_sampling = evaluate_model(temporal_model, eval_dataloader_no_sampling, DEVICE, NUM_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr2vK5M2ooHf",
        "outputId": "0362f198-6583-4cb1-dd10-d98d38d53816"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating without Selective Sampling\n",
            "Could not calculate ROC AUC: Number of classes in y_true not equal to the number of columns in 'y_score'. Check if all classes are present in predictions/labels.\n",
            "\n",
            "--- Frame-Level Metrics ---\n",
            "Accuracy: 0.9303\n",
            "Weighted Precision: 0.9169, Recall: 0.9303, F1-Score: 0.9188\n",
            "Micro Precision: 0.9303, Recall: 0.9303, F1-Score: 0.9303\n",
            "Weighted ROC AUC: nan\n",
            "Confusion Matrix:\n",
            " [[34168     0     0     0     0     0     0    94     0     0    68     8\n",
            "      0]\n",
            " [    0     0     0     0     0     0     0     0     0     0    10     0\n",
            "      0]\n",
            " [  123     0   582     0     0     0    19    41     0     0    90     0\n",
            "     11]\n",
            " [   17     0     8   337     0     0    30    27     0     0    39     0\n",
            "      0]\n",
            " [  312     0     0     0     0     0     0    84     0     0    98    13\n",
            "      0]\n",
            " [  146     0     0     0     0     0     0     9     0     0     0     4\n",
            "      0]\n",
            " [  213     0     0     0     0     0   502    37     0     0    19     0\n",
            "      5]\n",
            " [  190     0     0     6     0     0     0  3728     0     0     0   252\n",
            "     13]\n",
            " [  303     0     6     0     0     0    19     0   250     0    14     0\n",
            "      0]\n",
            " [    7     0     0    43     0     0     0     5     0     0     0     0\n",
            "      0]\n",
            " [  140     0     0     0     0     0    27     0     0     0  1370     0\n",
            "      1]\n",
            " [   64     0     0     0     0     0     0   147     0     0     0  2695\n",
            "      0]\n",
            " [  116     0     0     0     0     0     0   404     0     0     9     0\n",
            "    325]]\n",
            "\n",
            "--- Detected Lesions ---\n",
            "  Video: 04a78ef00c5245e0, Frame: 887, Time: 29.57s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 888, Time: 29.60s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 889, Time: 29.63s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 890, Time: 29.67s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 891, Time: 29.70s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 892, Time: 29.73s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 893, Time: 29.77s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 894, Time: 29.80s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 895, Time: 29.83s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 896, Time: 29.87s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 897, Time: 29.90s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 898, Time: 29.93s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 899, Time: 29.97s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 900, Time: 30.00s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 901, Time: 30.03s, Class: Pylorus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Sampling"
      ],
      "metadata": {
        "id": "mEwGs5ZBt1kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine"
      ],
      "metadata": {
        "id": "zSIAEhHz-uax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with sampling (using cosine similarity)\n",
        "sampling_config = {'method': 'cosine', 'threshold': 0.95}\n",
        "temporal_dataset_with_sampling = VideoFeatureDataset(non_empty_video_features, use_sampling=True, sampling_params=sampling_config)"
      ],
      "metadata": {
        "id": "nu5_GAXeta1c"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_with_sampling = DataLoader(\n",
        "    temporal_dataset_with_sampling,\n",
        "    batch_size=TEMPORAL_BATCH_SIZE,\n",
        "    shuffle=True, # Shuffle videos for training\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "print(f\"Temporal Dataloader (With Sampling): {len(dataloader_with_sampling)} batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tmKJkfLt_rE",
        "outputId": "b666199d-5ac1-40ae-a4fa-77889d884166"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporal Dataloader (With Sampling): 6 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Loop ---\n",
        "print(\"Starting Temporal Model Training (with sampling)...\")\n",
        "train_dataloader = dataloader_with_sampling\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = train_epoch(temporal_model, train_dataloader, criterion, optimizer, DEVICE)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbAGqpoguE7L",
        "outputId": "d0e8018c-955e-4efa-c56a-fc4e3254cac3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Temporal Model Training (with sampling)...\n",
            "Epoch 1/20, Loss: 0.8831\n",
            "Epoch 2/20, Loss: 0.9144\n",
            "Epoch 3/20, Loss: 0.8462\n",
            "Epoch 4/20, Loss: 0.8274\n",
            "Epoch 5/20, Loss: 0.8674\n",
            "Epoch 6/20, Loss: 0.8523\n",
            "Epoch 7/20, Loss: 0.8589\n",
            "Epoch 8/20, Loss: 0.6967\n",
            "Epoch 9/20, Loss: 0.6772\n",
            "Epoch 10/20, Loss: 0.6216\n",
            "Epoch 11/20, Loss: 0.7294\n",
            "Epoch 12/20, Loss: 0.7785\n",
            "Epoch 13/20, Loss: 0.8091\n",
            "Epoch 14/20, Loss: 0.7051\n",
            "Epoch 15/20, Loss: 0.6549\n",
            "Epoch 16/20, Loss: 0.6808\n",
            "Epoch 17/20, Loss: 0.7135\n",
            "Epoch 18/20, Loss: 0.6325\n",
            "Epoch 19/20, Loss: 0.6531\n",
            "Epoch 20/20, Loss: 0.6555\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(temporal_model.state_dict(), 'temporal_model_cosine_sampling95.pth')"
      ],
      "metadata": {
        "id": "nokA2wWbuNeH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('temporal_model_cosine_sampling95.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "q4QMm-7Q7ARC",
        "outputId": "c40b8e67-5ba9-47c0-efe0-e37944328425"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa2f5acc-4df4-456f-9ea1-7bc1a89fe8b1\", \"temporal_model_cosine_sampling95.pth\", 16845254)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Evaluating with Selective Sampling ({sampling_config['method']}, thresh={sampling_config['threshold']})\")\n",
        "eval_dataloader_with_sampling = DataLoader(\n",
        "    temporal_dataset_with_sampling, batch_size=TEMPORAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "detections_with_sampling, results_per_video_with_sampling = evaluate_model(temporal_model, eval_dataloader_with_sampling, DEVICE, NUM_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvmnaxOy8Gun",
        "outputId": "fd3c79b7-7ba2-4f09-b0b8-5be67dc2214e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with Selective Sampling (cosine, thresh=0.95)\n",
            "Could not calculate ROC AUC: Number of classes in y_true not equal to the number of columns in 'y_score'. Check if all classes are present in predictions/labels.\n",
            "\n",
            "--- Frame-Level Metrics ---\n",
            "Accuracy: 0.8985\n",
            "Weighted Precision: 0.8926, Recall: 0.8985, F1-Score: 0.8877\n",
            "Micro Precision: 0.8985, Recall: 0.8985, F1-Score: 0.8985\n",
            "Weighted ROC AUC: nan\n",
            "Confusion Matrix:\n",
            " [[11711     0     0     0     0     0     0   108     0     0   244    15\n",
            "      0]\n",
            " [    0     0     0     0     0     0     0     0     0     0    10     0\n",
            "      0]\n",
            " [   38     0   133     2     0     0     0    37     3     0    49     0\n",
            "      7]\n",
            " [    0     0     0   130     0     0     0    22     0     0    26     0\n",
            "      0]\n",
            " [  116     0     0     0     0     0     0    54     0     0    43     0\n",
            "      4]\n",
            " [   42     0     0     0     0     0     0     0    13     0     0     4\n",
            "      0]\n",
            " [   27     0     0     0     0     0   331    38     0     0    15     0\n",
            "      5]\n",
            " [   24     0     0     2     0     0     0  1531     2     0     0    53\n",
            "     13]\n",
            " [  103     0     0     0     0     0   122     0   109     0    32     0\n",
            "      0]\n",
            " [    5     0     0    30     0     0     0     5     0     0     0     0\n",
            "      0]\n",
            " [   38     0     0     0     0     0    15     0     0     0   698     0\n",
            "     12]\n",
            " [    2     0     0     0     0     0     0   216     0     0     0   468\n",
            "      0]\n",
            " [    0     0     0     0     0     0     0   132     0     0     0     0\n",
            "    194]]\n",
            "\n",
            "--- Detected Lesions ---\n",
            "  Video: 04a78ef00c5245e0, Frame: 887, Time: 29.57s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 888, Time: 29.60s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 895, Time: 29.83s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 896, Time: 29.87s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 898, Time: 29.93s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 902, Time: 30.07s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 2718, Time: 90.60s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 2720, Time: 90.67s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 2721, Time: 90.70s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 2722, Time: 90.73s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 2723, Time: 90.77s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 4372, Time: 145.73s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 4380, Time: 146.00s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 4382, Time: 146.07s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 4383, Time: 146.10s, Class: Pylorus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RRR"
      ],
      "metadata": {
        "id": "YI5tUeYV8rDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for vid, data in video_features.items():\n",
        "    features = data['features']\n",
        "    frame_ids = data['frame_ids']\n",
        "\n",
        "    _, sampled_features, sampled_frame_ids = selective_frame_sampling(\n",
        "        features, frame_ids,\n",
        "        method='cosine', threshold=0.95\n",
        "    )\n",
        "    print(f\"Video {vid}: selected {len(sampled_frame_ids)} / {len(frame_ids)} frames\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e3N5NR6gFt5D",
        "outputId": "8cc3b2db-5734-4226-8920-980632e74a07"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 04a78ef00c5245e0: selected 611 / 1292 frames\n",
            "Video 0531325b64674948: selected 360 / 1191 frames\n",
            "Video 0728084c8da942d9: selected 599 / 1499 frames\n",
            "Video 07c1fa15a20a4398: selected 214 / 678 frames\n",
            "Video 131368cc17e44240: selected 823 / 2034 frames\n",
            "Video 2fc3db471f9d44c0: selected 49 / 127 frames\n",
            "Video 39960e5e099a45ca: selected 93 / 237 frames\n",
            "Video 3ada4222967f421d: selected 405 / 1002 frames\n",
            "Video 3c8d5f0b90d7475d: selected 550 / 1704 frames\n",
            "Video 4560e83f9afc4685: selected 283 / 780 frames\n",
            "Video 48579eec79784294: selected 604 / 1379 frames\n",
            "Video 495f16498db34d3c: selected 182 / 323 frames\n",
            "Video 4aebc5cb2d4847aa: selected 64 / 90 frames\n",
            "Video 5bb1d3cc7dc64cec: selected 343 / 788 frames\n",
            "Video 5e59c7fdb16c4228: selected 385 / 2394 frames\n",
            "Video 5e9beaf4e66142c8: selected 80 / 464 frames\n",
            "Video 64440803f87b4843: selected 764 / 2159 frames\n",
            "Video 6cb700585c4f4070: selected 275 / 884 frames\n",
            "Video 7a47e8eacea04e64: selected 289 / 436 frames\n",
            "Video 7ad22d50ebaf4596: selected 312 / 740 frames\n",
            "Video 8885668afb844852: selected 759 / 2021 frames\n",
            "Video 89cdd41258c542c5: selected 473 / 1198 frames\n",
            "Video 8a00709108cd4e2b: selected 147 / 542 frames\n",
            "Video 8ebf0e483cac48d6: selected 376 / 1077 frames\n",
            "Video ad91cf7ca91440aa: selected 21 / 74 frames\n",
            "Video af9bd7d0e43741e3: selected 160 / 431 frames\n",
            "Video b2134f4a6f864613: selected 422 / 1297 frames\n",
            "Video bc84479c66fe4da6: selected 245 / 1891 frames\n",
            "Video bca26705313a4644: selected 946 / 1449 frames\n",
            "Video c11b28a8b2344716: selected 240 / 1000 frames\n",
            "Video c7084b3556e34619: selected 314 / 976 frames\n",
            "Video d369e4f163df4aba: selected 418 / 1328 frames\n",
            "Video d626f4f4a5ac4785: selected 229 / 643 frames\n",
            "Video d7a271f233ba4a40: selected 458 / 945 frames\n",
            "Video dac1e27f7e4d4ef5: selected 818 / 2152 frames\n",
            "Video dc221ccc65d34010: selected 486 / 1789 frames\n",
            "Video dca1377dec974312: selected 374 / 1216 frames\n",
            "Video df6b47bafe5143f5: selected 647 / 1530 frames\n",
            "Video eb0203196e284797: selected 308 / 881 frames\n",
            "Video ed02f27ef36f483e: selected 391 / 587 frames\n",
            "Video fb86bc87d3874cd7: selected 912 / 2086 frames\n",
            "Video fc32def0e7194981: selected 395 / 1380 frames\n",
            "Video fe5d372e43f94f68: selected 209 / 554 frames\n",
            "Video 04a78ef00c5245e0_11227.jpg: selected 0 / 0 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_selected_frames = 0\n",
        "for vid, data in video_features.items():\n",
        "    features = data['features']\n",
        "    frame_ids = data['frame_ids']\n",
        "\n",
        "    _, sampled_features, _ = selective_frame_sampling(\n",
        "        features, frame_ids,\n",
        "        method='cosine', threshold=0.95\n",
        "    )\n",
        "    total_selected_frames += len(sampled_features)\n",
        "\n",
        "print(f\"Total selected frames after cosine sampling: {total_selected_frames}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhFZDHotF6Wn",
        "outputId": "23094e10-3488-4ad7-e85b-4741b2b901a0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total selected frames after cosine sampling: 17033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_original_frames = len(df_meta)\n",
        "rrr = calculate_rrr(total_original_frames, total_selected_frames)\n",
        "print(f\"Redundancy Reduction Ratio (RRR) for sampling run: {rrr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnUWUuk28QSp",
        "outputId": "cd66d410-4ba1-43d7-849f-548ce37f5b5f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Redundancy Reduction Ratio (RRR) for sampling run: 0.6395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total original frames: {total_original_frames}\")\n",
        "print(f\"Total sampled frames: {total_selected_frames}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnIumTVU8wZI",
        "outputId": "beffed93-289c-4a16-e78e-bb3ba0285e5a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total original frames: 47248\n",
            "Total sampled frames: 17033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "p2nMpxL888yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(detections_with_sampling[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAvLY7jYGUzl",
        "outputId": "85db8973-bb87-4afb-a459-0b126ebf81cc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'video_id': '04a78ef00c5245e0', 'frame_id': np.int64(887), 'timestamp_sec': np.float64(29.566666666666666), 'predicted_class_idx': np.int64(2), 'predicted_class_name': 'Angiectasia'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_detections(detections_with_sampling, IMAGE_BASE_PATH, df_meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owRSM6u9818_",
        "outputId": "91ceee69-08f7-4fb9-d166-956ca78d2f81"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating visualizations for 4927 detected lesion frames...\n",
            "  Saved 50 annotated frames...\n",
            "  Saved 100 annotated frames...\n",
            "  Saved 150 annotated frames...\n",
            "  Saved 200 annotated frames...\n",
            "  Saved 250 annotated frames...\n",
            "  Saved 300 annotated frames...\n",
            "  Saved 350 annotated frames...\n",
            "  Saved 400 annotated frames...\n",
            "  Saved 450 annotated frames...\n",
            "  Saved 500 annotated frames...\n",
            "  Saved 550 annotated frames...\n",
            "  Saved 600 annotated frames...\n",
            "  Saved 650 annotated frames...\n",
            "  Saved 700 annotated frames...\n",
            "  Saved 750 annotated frames...\n",
            "  Saved 800 annotated frames...\n",
            "  Saved 850 annotated frames...\n",
            "  Saved 900 annotated frames...\n",
            "  Saved 950 annotated frames...\n",
            "  Saved 1000 annotated frames...\n",
            "  Saved 1050 annotated frames...\n",
            "  Saved 1100 annotated frames...\n",
            "  Saved 1150 annotated frames...\n",
            "  Saved 1200 annotated frames...\n",
            "  Saved 1250 annotated frames...\n",
            "  Saved 1300 annotated frames...\n",
            "  Saved 1350 annotated frames...\n",
            "  Saved 1400 annotated frames...\n",
            "  Saved 1450 annotated frames...\n",
            "  Saved 1500 annotated frames...\n",
            "  Saved 1550 annotated frames...\n",
            "  Saved 1600 annotated frames...\n",
            "  Saved 1650 annotated frames...\n",
            "  Saved 1700 annotated frames...\n",
            "  Saved 1750 annotated frames...\n",
            "  Saved 1800 annotated frames...\n",
            "  Saved 1850 annotated frames...\n",
            "  Saved 1900 annotated frames...\n",
            "  Saved 1950 annotated frames...\n",
            "  Saved 2000 annotated frames...\n",
            "  Saved 2050 annotated frames...\n",
            "  Saved 2100 annotated frames...\n",
            "  Saved 2150 annotated frames...\n",
            "  Saved 2200 annotated frames...\n",
            "  Saved 2250 annotated frames...\n",
            "  Saved 2300 annotated frames...\n",
            "  Saved 2350 annotated frames...\n",
            "  Saved 2400 annotated frames...\n",
            "  Saved 2450 annotated frames...\n",
            "  Saved 2500 annotated frames...\n",
            "  Saved 2550 annotated frames...\n",
            "  Saved 2600 annotated frames...\n",
            "  Saved 2650 annotated frames...\n",
            "  Saved 2700 annotated frames...\n",
            "  Saved 2750 annotated frames...\n",
            "  Saved 2800 annotated frames...\n",
            "  Saved 2850 annotated frames...\n",
            "  Saved 2900 annotated frames...\n",
            "  Saved 2950 annotated frames...\n",
            "  Saved 3000 annotated frames...\n",
            "  Saved 3050 annotated frames...\n",
            "  Saved 3100 annotated frames...\n",
            "  Saved 3150 annotated frames...\n",
            "  Saved 3200 annotated frames...\n",
            "  Saved 3250 annotated frames...\n",
            "  Saved 3300 annotated frames...\n",
            "  Saved 3350 annotated frames...\n",
            "  Saved 3400 annotated frames...\n",
            "  Saved 3450 annotated frames...\n",
            "  Saved 3500 annotated frames...\n",
            "  Saved 3550 annotated frames...\n",
            "  Saved 3600 annotated frames...\n",
            "  Saved 3650 annotated frames...\n",
            "  Saved 3700 annotated frames...\n",
            "  Saved 3750 annotated frames...\n",
            "  Saved 3800 annotated frames...\n",
            "  Saved 3850 annotated frames...\n",
            "  Saved 3900 annotated frames...\n",
            "  Saved 3950 annotated frames...\n",
            "  Saved 4000 annotated frames...\n",
            "  Saved 4050 annotated frames...\n",
            "  Saved 4100 annotated frames...\n",
            "  Saved 4150 annotated frames...\n",
            "  Saved 4200 annotated frames...\n",
            "  Saved 4250 annotated frames...\n",
            "  Saved 4300 annotated frames...\n",
            "  Saved 4350 annotated frames...\n",
            "  Saved 4400 annotated frames...\n",
            "  Saved 4450 annotated frames...\n",
            "  Saved 4500 annotated frames...\n",
            "  Saved 4550 annotated frames...\n",
            "  Saved 4600 annotated frames...\n",
            "  Saved 4650 annotated frames...\n",
            "  Saved 4700 annotated frames...\n",
            "  Saved 4750 annotated frames...\n",
            "  Saved 4800 annotated frames...\n",
            "  Saved 4850 annotated frames...\n",
            "  Saved 4900 annotated frames...\n",
            "Finished saving 4927 annotated lesion frames to 'visualization'.\n",
            "Saved detection summary report to 'detection_summary_cv.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDR"
      ],
      "metadata": {
        "id": "IMgI__CA9L6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_video_labels = defaultdict(bool)\n",
        "negative_label_index = LABEL_MAP.get('Normal clean mucosa', LABEL_MAP.get('Normal clean mucosa', 0))\n",
        "for index, row in df_meta.iterrows():\n",
        "    if row['label_idx'] != negative_label_index:\n",
        "         ground_truth_video_labels[row['video_id']] = True\n",
        "\n",
        "ground_truth_video_labels = dict(ground_truth_video_labels)\n",
        "\n",
        "print(f\"\\nCalculating LDR for evaluation run...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkg_IWjW9Ayp",
        "outputId": "663805ce-258d-4196-fd41-aa498346ca88"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating LDR for evaluation run...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ldr_no_sampling = calculate_ldr(results_per_video_no_sampling, ground_truth_video_labels, LABEL_MAP)\n",
        "print(f\"LDR (No Sampling): {ldr_no_sampling:.4f}\")\n",
        "\n",
        "ldr_value = calculate_ldr(results_per_video_with_sampling, ground_truth_video_labels, LABEL_MAP)\n",
        "print(f\"LDR (with sampling): {ldr_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk8b5_Sv9moj",
        "outputId": "32c6d333-7b99-4db8-ba0d-4055a1a9378e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDR (No Sampling): 1.0000\n",
            "LDR (with sampling): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With sampling- euclidean"
      ],
      "metadata": {
        "id": "611UX-OkHWYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_config = {'method': 'euclidean', 'threshold': 0.5}\n",
        "temporal_dataset_w_sampling = VideoFeatureDataset(non_empty_video_features, use_sampling=True, sampling_params=sampling_config)"
      ],
      "metadata": {
        "id": "AnZTkAFp-Uv0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_w_sampling = DataLoader(\n",
        "    temporal_dataset_w_sampling,\n",
        "    batch_size=TEMPORAL_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "print(f\"Temporal Dataloader (With Sampling): {len(dataloader_w_sampling)} batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l05bNwIdHgYl",
        "outputId": "60c05633-9da1-4df4-b590-14118ec38665"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporal Dataloader (With Sampling): 6 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Loop ---\n",
        "print(\"Starting Temporal Model Training (with sampling)...\")\n",
        "train_dataloader = dataloader_w_sampling\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = train_epoch(temporal_model, train_dataloader, criterion, optimizer, DEVICE)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POXZW2DhHnoi",
        "outputId": "e2ee1f59-f5eb-4d0c-c1be-bf0d0f5c3cb2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Temporal Model Training (with sampling)...\n",
            "Epoch 1/20, Loss: 0.3094\n",
            "Epoch 2/20, Loss: 0.2694\n",
            "Epoch 3/20, Loss: 0.2456\n",
            "Epoch 4/20, Loss: 0.2719\n",
            "Epoch 5/20, Loss: 0.2458\n",
            "Epoch 6/20, Loss: 0.2315\n",
            "Epoch 7/20, Loss: 0.2505\n",
            "Epoch 8/20, Loss: 0.1942\n",
            "Epoch 9/20, Loss: 0.1891\n",
            "Epoch 10/20, Loss: 0.1904\n",
            "Epoch 11/20, Loss: 0.1937\n",
            "Epoch 12/20, Loss: 0.2139\n",
            "Epoch 13/20, Loss: 0.1772\n",
            "Epoch 14/20, Loss: 0.1823\n",
            "Epoch 15/20, Loss: 0.1779\n",
            "Epoch 16/20, Loss: 0.1903\n",
            "Epoch 17/20, Loss: 0.4146\n",
            "Epoch 18/20, Loss: 0.3338\n",
            "Epoch 19/20, Loss: 0.3624\n",
            "Epoch 20/20, Loss: 0.3760\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(temporal_model.state_dict(), 'temporal_model_ed_sampling05.pth')"
      ],
      "metadata": {
        "id": "vo9wDKZnHt5c"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('temporal_model_ed_sampling05.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CHf2HmrEPfna",
        "outputId": "fa398c63-f853-487f-fbe8-009f1375658c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_72d73922-ee83-4379-82db-90dc98013ebc\", \"temporal_model_ed_sampling05.pth\", 16845166)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Evaluating with Selective Sampling ({sampling_config['method']}, thresh={sampling_config['threshold']})\")\n",
        "eval_dataloader_w_sampling = DataLoader(\n",
        "    temporal_dataset_w_sampling, batch_size=TEMPORAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "detections_w_sampling, results_per_video_w_sampling = evaluate_model(temporal_model, eval_dataloader_w_sampling, DEVICE, NUM_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7AJHU_nPjGw",
        "outputId": "ce2b871c-4a73-41f5-ee6f-d350871dd05b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with Selective Sampling (euclidean, thresh=0.5)\n",
            "Could not calculate ROC AUC: Number of classes in y_true not equal to the number of columns in 'y_score'. Check if all classes are present in predictions/labels.\n",
            "\n",
            "--- Frame-Level Metrics ---\n",
            "Accuracy: 0.9084\n",
            "Weighted Precision: 0.8828, Recall: 0.9084, F1-Score: 0.8831\n",
            "Micro Precision: 0.9084, Recall: 0.9084, F1-Score: 0.9084\n",
            "Weighted ROC AUC: nan\n",
            "Confusion Matrix:\n",
            " [[34194     0     0     0     0     0     0    38     0     0    53    53\n",
            "      0]\n",
            " [    0     0     0     0     0     0     0     0     0     0    10     0\n",
            "      0]\n",
            " [  594     0    15     0     0     0     0    40   108     0   101     0\n",
            "      8]\n",
            " [   39     0     0     0     0     0     0   182     0     0    44     0\n",
            "    193]\n",
            " [  341     0     0     0     0     0     0    82     0     0    69    14\n",
            "      0]\n",
            " [  149     0     0     0     0     0     0     0     0     0     0    10\n",
            "      0]\n",
            " [  268     0     0     0     0     0   382    34     0     0    89     0\n",
            "      3]\n",
            " [  135     0     0     0     0     0     0  3790     0     0     0   252\n",
            "     12]\n",
            " [  464     0     0     0     0     0     4    28    96     0     0     0\n",
            "      0]\n",
            " [    0     0     0     0     0     0     0    55     0     0     0     0\n",
            "      0]\n",
            " [  251     0     0     0     0     0     0     0     0     0  1263     2\n",
            "      4]\n",
            " [    1     0     0     0     0     0     0    83     0     0     0  2822\n",
            "      0]\n",
            " [   80     0     0     0     0     0     0   414     0     0     0    21\n",
            "    339]]\n",
            "\n",
            "--- Detected Lesions ---\n",
            "  Video: 04a78ef00c5245e0, Frame: 887, Time: 29.57s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 888, Time: 29.60s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 889, Time: 29.63s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 890, Time: 29.67s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 891, Time: 29.70s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 892, Time: 29.73s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 893, Time: 29.77s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 894, Time: 29.80s, Class: Angiectasia\n",
            "  Video: 04a78ef00c5245e0, Frame: 895, Time: 29.83s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 896, Time: 29.87s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 897, Time: 29.90s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 898, Time: 29.93s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 899, Time: 29.97s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 900, Time: 30.00s, Class: Pylorus\n",
            "  Video: 04a78ef00c5245e0, Frame: 901, Time: 30.03s, Class: Pylorus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RRR"
      ],
      "metadata": {
        "id": "wxivn3a7QMMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_selected_frames = 0\n",
        "for vid, data in video_features.items():\n",
        "    features = data['features']\n",
        "    frame_ids = data['frame_ids']\n",
        "\n",
        "    _, sampled_features, _ = selective_frame_sampling(\n",
        "        features, frame_ids,\n",
        "        method='euclidean', threshold=0.5\n",
        "    )\n",
        "    total_selected_frames += len(sampled_features)\n",
        "\n",
        "print(f\"Total selected frames after euclidean sampling: {total_selected_frames}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K8ef6_QQAC9",
        "outputId": "ac4745f1-13c4-4b12-fadc-ea76d5291a26"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total selected frames after euclidean sampling: 47229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_original_frames = len(df_meta)\n",
        "rrr = calculate_rrr(total_original_frames, total_selected_frames)\n",
        "print(f\"Redundancy Reduction Ratio (RRR) for sampling run: {rrr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QgTDRBDQKvm",
        "outputId": "95710af9-51f2-4b06-d9a8-70571021b1a3"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Redundancy Reduction Ratio (RRR) for sampling run: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total original frames: {total_original_frames}\")\n",
        "print(f\"Total sampled frames: {total_selected_frames}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFn8toNZQWzG",
        "outputId": "e6da90a1-ab4b-4c22-cc9f-136ce4304c7b"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total original frames: 47248\n",
            "Total sampled frames: 47229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDR"
      ],
      "metadata": {
        "id": "PVnXXA60QykS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_video_labels = defaultdict(bool)\n",
        "negative_label_index = LABEL_MAP.get('Normal clean mucosa', LABEL_MAP.get('Normal clean mucosa', 0))\n",
        "for index, row in df_meta.iterrows():\n",
        "    if row['label_idx'] != negative_label_index:\n",
        "         ground_truth_video_labels[row['video_id']] = True\n",
        "\n",
        "ground_truth_video_labels = dict(ground_truth_video_labels)"
      ],
      "metadata": {
        "id": "yUfWbsDHQxnp"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldr_value = calculate_ldr(results_per_video_w_sampling, ground_truth_video_labels, LABEL_MAP)\n",
        "print(f\"LDR (with sampling): {ldr_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJfsm20pQ6oj",
        "outputId": "1cb2d48c-1a4f-4c87-86a3-9757805962c3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDR (with sampling): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization and Summary"
      ],
      "metadata": {
        "id": "O9hk2kZlRDh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_detections(detections_w_sampling, IMAGE_BASE_PATH, df_meta)\n",
        "create_summary_report(detections_w_sampling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYGLr0SCREYr",
        "outputId": "26841db1-e8b7-442c-919d-c86695858247"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating visualizations for 10713 detected lesion frames...\n",
            "  Saved 50 annotated frames...\n",
            "  Saved 100 annotated frames...\n",
            "  Saved 150 annotated frames...\n",
            "  Saved 200 annotated frames...\n",
            "  Saved 250 annotated frames...\n",
            "  Saved 300 annotated frames...\n",
            "  Saved 350 annotated frames...\n",
            "  Saved 400 annotated frames...\n",
            "  Saved 450 annotated frames...\n",
            "  Saved 500 annotated frames...\n",
            "  Saved 550 annotated frames...\n",
            "  Saved 600 annotated frames...\n",
            "  Saved 650 annotated frames...\n",
            "  Saved 700 annotated frames...\n",
            "  Saved 750 annotated frames...\n",
            "  Saved 800 annotated frames...\n",
            "  Saved 850 annotated frames...\n",
            "  Saved 900 annotated frames...\n",
            "  Saved 950 annotated frames...\n",
            "  Saved 1000 annotated frames...\n",
            "  Saved 1050 annotated frames...\n",
            "  Saved 1100 annotated frames...\n",
            "  Saved 1150 annotated frames...\n",
            "  Saved 1200 annotated frames...\n",
            "  Saved 1250 annotated frames...\n",
            "  Saved 1300 annotated frames...\n",
            "  Saved 1350 annotated frames...\n",
            "  Saved 1400 annotated frames...\n",
            "  Saved 1450 annotated frames...\n",
            "  Saved 1500 annotated frames...\n",
            "  Saved 1550 annotated frames...\n",
            "  Saved 1600 annotated frames...\n",
            "  Saved 1650 annotated frames...\n",
            "  Saved 1700 annotated frames...\n",
            "  Saved 1750 annotated frames...\n",
            "  Saved 1800 annotated frames...\n",
            "  Saved 1850 annotated frames...\n",
            "  Saved 1900 annotated frames...\n",
            "  Saved 1950 annotated frames...\n",
            "  Saved 2000 annotated frames...\n",
            "  Saved 2050 annotated frames...\n",
            "  Saved 2100 annotated frames...\n",
            "  Saved 2150 annotated frames...\n",
            "  Saved 2200 annotated frames...\n",
            "  Saved 2250 annotated frames...\n",
            "  Saved 2300 annotated frames...\n",
            "  Saved 2350 annotated frames...\n",
            "  Saved 2400 annotated frames...\n",
            "  Saved 2450 annotated frames...\n",
            "  Saved 2500 annotated frames...\n",
            "  Saved 2550 annotated frames...\n",
            "  Saved 2600 annotated frames...\n",
            "  Saved 2650 annotated frames...\n",
            "  Saved 2700 annotated frames...\n",
            "  Saved 2750 annotated frames...\n",
            "  Saved 2800 annotated frames...\n",
            "  Saved 2850 annotated frames...\n",
            "  Saved 2900 annotated frames...\n",
            "  Saved 2950 annotated frames...\n",
            "  Saved 3000 annotated frames...\n",
            "  Saved 3050 annotated frames...\n",
            "  Saved 3100 annotated frames...\n",
            "  Saved 3150 annotated frames...\n",
            "  Saved 3200 annotated frames...\n",
            "  Saved 3250 annotated frames...\n",
            "  Saved 3300 annotated frames...\n",
            "  Saved 3350 annotated frames...\n",
            "  Saved 3400 annotated frames...\n",
            "  Saved 3450 annotated frames...\n",
            "  Saved 3500 annotated frames...\n",
            "  Saved 3550 annotated frames...\n",
            "  Saved 3600 annotated frames...\n",
            "  Saved 3650 annotated frames...\n",
            "  Saved 3700 annotated frames...\n",
            "  Saved 3750 annotated frames...\n",
            "  Saved 3800 annotated frames...\n",
            "  Saved 3850 annotated frames...\n",
            "  Saved 3900 annotated frames...\n",
            "  Saved 3950 annotated frames...\n",
            "  Saved 4000 annotated frames...\n",
            "  Saved 4050 annotated frames...\n",
            "  Saved 4100 annotated frames...\n",
            "  Saved 4150 annotated frames...\n",
            "  Saved 4200 annotated frames...\n",
            "  Saved 4250 annotated frames...\n",
            "  Saved 4300 annotated frames...\n",
            "  Saved 4350 annotated frames...\n",
            "  Saved 4400 annotated frames...\n",
            "  Saved 4450 annotated frames...\n",
            "  Saved 4500 annotated frames...\n",
            "  Saved 4550 annotated frames...\n",
            "  Saved 4600 annotated frames...\n",
            "  Saved 4650 annotated frames...\n",
            "  Saved 4700 annotated frames...\n",
            "  Saved 4750 annotated frames...\n",
            "  Saved 4800 annotated frames...\n",
            "  Saved 4850 annotated frames...\n",
            "  Saved 4900 annotated frames...\n",
            "  Saved 4950 annotated frames...\n",
            "  Saved 5000 annotated frames...\n",
            "  Saved 5050 annotated frames...\n",
            "  Saved 5100 annotated frames...\n",
            "  Saved 5150 annotated frames...\n",
            "  Saved 5200 annotated frames...\n",
            "  Saved 5250 annotated frames...\n",
            "  Saved 5300 annotated frames...\n",
            "  Saved 5350 annotated frames...\n",
            "  Saved 5400 annotated frames...\n",
            "  Saved 5450 annotated frames...\n",
            "  Saved 5500 annotated frames...\n",
            "  Saved 5550 annotated frames...\n",
            "  Saved 5600 annotated frames...\n",
            "  Saved 5650 annotated frames...\n",
            "  Saved 5700 annotated frames...\n",
            "  Saved 5750 annotated frames...\n",
            "  Saved 5800 annotated frames...\n",
            "  Saved 5850 annotated frames...\n",
            "  Saved 5900 annotated frames...\n",
            "  Saved 5950 annotated frames...\n",
            "  Saved 6000 annotated frames...\n",
            "  Saved 6050 annotated frames...\n",
            "  Saved 6100 annotated frames...\n",
            "  Saved 6150 annotated frames...\n",
            "  Saved 6200 annotated frames...\n",
            "  Saved 6250 annotated frames...\n",
            "  Saved 6300 annotated frames...\n",
            "  Saved 6350 annotated frames...\n",
            "  Saved 6400 annotated frames...\n",
            "  Saved 6450 annotated frames...\n",
            "  Saved 6500 annotated frames...\n",
            "  Saved 6550 annotated frames...\n",
            "  Saved 6600 annotated frames...\n",
            "  Saved 6650 annotated frames...\n",
            "  Saved 6700 annotated frames...\n",
            "  Saved 6750 annotated frames...\n",
            "  Saved 6800 annotated frames...\n",
            "  Saved 6850 annotated frames...\n",
            "  Saved 6900 annotated frames...\n",
            "  Saved 6950 annotated frames...\n",
            "  Saved 7000 annotated frames...\n",
            "  Saved 7050 annotated frames...\n",
            "  Saved 7100 annotated frames...\n",
            "  Saved 7150 annotated frames...\n",
            "  Saved 7200 annotated frames...\n",
            "  Saved 7250 annotated frames...\n",
            "  Saved 7300 annotated frames...\n",
            "  Saved 7350 annotated frames...\n",
            "  Saved 7400 annotated frames...\n",
            "  Saved 7450 annotated frames...\n",
            "  Saved 7500 annotated frames...\n",
            "  Saved 7550 annotated frames...\n",
            "  Saved 7600 annotated frames...\n",
            "  Saved 7650 annotated frames...\n",
            "  Saved 7700 annotated frames...\n",
            "  Saved 7750 annotated frames...\n",
            "  Saved 7800 annotated frames...\n",
            "  Saved 7850 annotated frames...\n",
            "  Saved 7900 annotated frames...\n",
            "  Saved 7950 annotated frames...\n",
            "  Saved 8000 annotated frames...\n",
            "  Saved 8050 annotated frames...\n",
            "  Saved 8100 annotated frames...\n",
            "  Saved 8150 annotated frames...\n",
            "  Saved 8200 annotated frames...\n",
            "  Saved 8250 annotated frames...\n",
            "  Saved 8300 annotated frames...\n",
            "  Saved 8350 annotated frames...\n",
            "  Saved 8400 annotated frames...\n",
            "  Saved 8450 annotated frames...\n",
            "  Saved 8500 annotated frames...\n",
            "  Saved 8550 annotated frames...\n",
            "  Saved 8600 annotated frames...\n",
            "  Saved 8650 annotated frames...\n",
            "  Saved 8700 annotated frames...\n",
            "  Saved 8750 annotated frames...\n",
            "  Saved 8800 annotated frames...\n",
            "  Saved 8850 annotated frames...\n",
            "  Saved 8900 annotated frames...\n",
            "  Saved 8950 annotated frames...\n",
            "  Saved 9000 annotated frames...\n",
            "  Saved 9050 annotated frames...\n",
            "  Saved 9100 annotated frames...\n",
            "  Saved 9150 annotated frames...\n",
            "  Saved 9200 annotated frames...\n",
            "  Saved 9250 annotated frames...\n",
            "  Saved 9300 annotated frames...\n",
            "  Saved 9350 annotated frames...\n",
            "  Saved 9400 annotated frames...\n",
            "  Saved 9450 annotated frames...\n",
            "  Saved 9500 annotated frames...\n",
            "  Saved 9550 annotated frames...\n",
            "  Saved 9600 annotated frames...\n",
            "  Saved 9650 annotated frames...\n",
            "  Saved 9700 annotated frames...\n",
            "  Saved 9750 annotated frames...\n",
            "  Saved 9800 annotated frames...\n",
            "  Saved 9850 annotated frames...\n",
            "  Saved 9900 annotated frames...\n",
            "  Saved 9950 annotated frames...\n",
            "  Saved 10000 annotated frames...\n",
            "  Saved 10050 annotated frames...\n",
            "  Saved 10100 annotated frames...\n",
            "  Saved 10150 annotated frames...\n",
            "  Saved 10200 annotated frames...\n",
            "  Saved 10250 annotated frames...\n",
            "  Saved 10300 annotated frames...\n",
            "  Saved 10350 annotated frames...\n",
            "  Saved 10400 annotated frames...\n",
            "  Saved 10450 annotated frames...\n",
            "  Saved 10500 annotated frames...\n",
            "  Saved 10550 annotated frames...\n",
            "  Saved 10600 annotated frames...\n",
            "  Saved 10650 annotated frames...\n",
            "  Saved 10700 annotated frames...\n",
            "Finished saving 10713 annotated lesion frames to 'visualization'.\n",
            "Saved detection summary report to 'detection_summary_cv.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = '/content/visualization'\n",
        "destination_folder = '/content/drive/My Drive/ZippedBackups/visualizationCosineEuclidean'\n",
        "\n",
        "shutil.copytree(source_folder, destination_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3s-d6rqfRgzr",
        "outputId": "c47d73d4-1c8f-483e-b04c-41fc91258f30"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/ZippedBackups/visualizationCosineEuclidean'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZkvjVk4TIgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}